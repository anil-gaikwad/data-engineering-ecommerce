# ğŸ›’ E-Commerce Analytics Data Platform
# ğŸ“Œ Overview

This project is an end-to-end Data Engineering pipeline built with Python + PySpark.
It simulates a real-world E-commerce Data Platform, handling both batch and streaming data from multiple sources:

ğŸ—‚ï¸ Transactions (CSV files â€“ batch ingestion)

ğŸ§‘â€ğŸ¤â€ğŸ§‘ Customer Data (PostgreSQL â€“ batch ingestion)

âš¡ Clickstream Logs (Kafka â€“ real-time streaming)

The pipeline processes data into Bronze â†’ Silver â†’ Gold layers (raw â†’ cleaned â†’ analytics-ready) and enables insights such as daily sales, active users, and customer behavior.


âš™ï¸ Architecture
```diff
          +-------------+
          |  PostgreSQL |
          +-------------+
                 |
        +-------------------+      +----------------+
        |   Kafka Stream    | ---> | Clickstream    |
        | (User Activity)   |      | Logs (JSON)    |
        +-------------------+      +----------------+
                 |
                 v
        =======================
         Ingestion Layer
        =======================
                 |
                 v
        =======================
         Processing Layer
        (PySpark ETL)
        =======================
         Bronze â†’ Silver â†’ Gold
                 |
                 v
        =======================
         Storage Layer
        (Delta Lake / Parquet)
        =======================
                 |
                 v
        =======================
         Analytics Layer
        (Superset / BI Tool)
        =======================
```


ğŸ“‚ Project Structure

```bash
ecommerce_data_platform/
â”‚â”€â”€ configs/
â”‚    â”œâ”€â”€ spark_config.yaml
â”‚    â””â”€â”€ db_config.yaml
â”‚
â”‚â”€â”€ data/
â”‚    â”œâ”€â”€ raw/          # Raw files (CSV, JSON)
â”‚    â”œâ”€â”€ bronze/       # Raw ingested data
â”‚    â”œâ”€â”€ silver/       # Cleaned & transformed
â”‚    â””â”€â”€ gold/         # Aggregated analytics
â”‚
â”‚â”€â”€ src/
â”‚    â”œâ”€â”€ ingestion/
â”‚    â”‚    â”œâ”€â”€ load_transactions.py
â”‚    â”‚    â”œâ”€â”€ load_customers.py
â”‚    â”‚    â””â”€â”€ stream_clickstream.py
â”‚    â”‚
â”‚    â”œâ”€â”€ processing/
â”‚    â”‚    â”œâ”€â”€ transform_transactions.py
â”‚    â”‚    â”œâ”€â”€ enrich_data.py
â”‚    â”‚    â””â”€â”€ aggregate_metrics.py
â”‚    â”‚
â”‚    â”œâ”€â”€ utils/
â”‚    â”‚    â”œâ”€â”€ logger.py
â”‚    â”‚    â”œâ”€â”€ config_loader.py
â”‚    â”‚    â””â”€â”€ db_connector.py
â”‚    â”‚
â”‚    â”œâ”€â”€ main_batch.py      # Batch pipeline entry
â”‚    â””â”€â”€ main_stream.py     # Streaming pipeline entry
â”‚
â”‚â”€â”€ docker-compose.yml      # Local PostgreSQL + Kafka
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md

```

ğŸš€ Features

* âœ… Batch ingestion of CSV + PostgreSQL data
* âœ… Real-time ingestion of Kafka clickstream logs
* âœ… ETL pipeline with PySpark (Bronze â†’ Silver â†’ Gold)
* âœ… Aggregated business KPIs (daily sales, unique customers, product demand)
* âœ… Streaming analytics for user behavior
* âœ… Pluggable storage (Parquet / Delta Lake)
* âœ… Ready for orchestration with Airflow / Prefect
* âœ… Can scale to AWS EMR / Databricks


ğŸ”§ Setup & Installation

1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/anil-gaikwad/data-engineering-ecommerce.git
cd ecommerce_data_platform
```

2ï¸âƒ£ Install Dependencies
```bash 
pip install -r requirements.txt

```
3ï¸âƒ£ Start Services (PostgreSQL + Kafka)
```bash
docker-compose up -d
```
4ï¸âƒ£ Load Sample Data

* Place sample transactions.csv inside data/raw/
* PostgreSQL customers table will be auto-created by docker-compose (or run SQL script manually).

â–¶ï¸ Running the Pipelines

ğŸ”¹ Batch Processing (Transactions + Customers)
```bash
python src/main_batch.py
```

* Reads raw transactions + customers

* Cleans & enriches data

* Saves processed data in data/silver/

* Writes daily sales KPIs in data/gold/analytics/

ğŸ”¹ Streaming Processing (Clickstream from Kafka)

```bash
python src/main_stream.py

```

* Reads JSON clickstream events from Kafka

* Processes in real-time

* Outputs to console / storage


ğŸ‘¨â€ğŸ’» Tech Stack

* Python 3.9+
* PySpark
* PostgreSQL (via JDBC)
* Apache Kafka
* Docker + docker-compose
* Parquet / Delta Lake
